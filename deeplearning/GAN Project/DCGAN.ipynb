{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ROx6ye5S6is3",
    "outputId": "f79dc05c-91cb-4ff6-c6a9-72099ea839c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  conference_room_train_lmdb.zip\n",
      "   creating: conference_room_train_lmdb/\n",
      "  inflating: conference_room_train_lmdb/lock.mdb  \n",
      "  inflating: conference_room_train_lmdb/data.mdb  \n",
      "mv: cannot stat 'conference_room_val_lmdb': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/fyu/lsun.git\n",
    "!python2.7 lsun/download.py -c conference_room\n",
    "!unzip conference_room_train_lmdb\n",
    "!unzip conference_room_val_lmdb\n",
    "!mkdir data\n",
    "!mv conference_room_train_lmdb data\n",
    "!mv conference_room_val_lmdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3tMMdzR66C1i",
    "outputId": "bef18140-d155-42e3-a92c-b4d2ee61136d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073750016 bytes == 0x58e5a000 @  0x7f156aca42a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLbq1kO35cSU"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--dataset', required=True, help='cifar10 | lsun | mnist |imagenet | folder | lfw | fake')\n",
    "#parser.add_argument('--dataroot', required=True, help='path to dataset')\n",
    "#parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "#parser.add_argument('--batchSize', type=int, default=64, help='input batch size')\n",
    "#parser.add_argument('--imageSize', type=int, default=64, help='the height / width of the input image to network')\n",
    "#parser.add_argument('--nz', type=int, default=100, help='size of the latent z vector')\n",
    "#parser.add_argument('--ngf', type=int, default=64)\n",
    "#parser.add_argument('--ndf', type=int, default=64)\n",
    "#parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "#parser.add_argument('--lr', type=float, default=0.0002, help='learning rate, default=0.0002')\n",
    "#parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "#parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "#parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "#parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "#parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "#parser.add_argument('--outf', default='.', help='folder to output images and model checkpoints')\n",
    "#parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "\n",
    "dataset_name = 'lsun'\n",
    "dataroot = 'data'\n",
    "workers = 2\n",
    "batchSize = 64\n",
    "imageSize = 64\n",
    "nz = 100\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "niter = 25\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ngpu = 1\n",
    "netGPath = ''\n",
    "netDPath = ''\n",
    "outf = ''\n",
    "torch.manual_seed(1)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "if dataset_name in ['imagenet', 'folder', 'lfw']:\n",
    "    # folder dataset\n",
    "    dataset = dset.ImageFolder(root=dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(imageSize),\n",
    "                                   transforms.CenterCrop(imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    nc=3\n",
    "elif dataset_name == 'lsun':\n",
    "    dataset = dset.LSUN(root=dataroot, classes=['conference_room_train'],\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(imageSize),\n",
    "                            transforms.CenterCrop(imageSize),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "    nc=3\n",
    "elif dataset_name == 'cifar10':\n",
    "    dataset = dset.CIFAR10(root=dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    nc=3\n",
    "\n",
    "elif dataset_name == 'mnist':\n",
    "        dataset = dset.MNIST(root=dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,)),\n",
    "                           ]))\n",
    "        nc=1\n",
    "\n",
    "elif dataset_name == 'fake':\n",
    "    dataset = dset.FakeData(image_size=(3, imageSize, imageSize),\n",
    "                            transform=transforms.ToTensor())\n",
    "    nc=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9vt6apD5exn"
   },
   "outputs": [],
   "source": [
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, num_workers=int(workers))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "ngpu = int(ngpu)\n",
    "nz = int(nz)\n",
    "ngf = int(ngf)\n",
    "ndf = int(ndf)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "BroK2gN35uTj",
    "outputId": "dd896824-adda-41fd-a9d3-67524c4a2125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "if netGPath != '':\n",
    "    netG.load_state_dict(torch.load(netGPath))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "t9iX_YJo5wSd",
    "outputId": "c18c21e1-d803-4ff8-ccce-3e12fe20ccb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "if netDPath != '':\n",
    "    netD.load_state_dict(torch.load(netDPath))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sl3o2Vz3KoQK",
    "outputId": "65d33ae7-a5f9-45d9-8fac-64c158b194b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow==4.1.1 in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow==4.1.1) (0.46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pillow==4.1.1\n",
    "%reload_ext autoreload\n",
    "%autoreload\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7633
    },
    "colab_type": "code",
    "id": "-cX2bAsh587W",
    "outputId": "bc05f8e6-be3d-4033-cd71-243fff076da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/3580] Loss_D: 0.2428 Loss_G: 0.3947 D(x): 0.8446 D(G(z)): 0.0439 / 0.7338\n",
      "[0/25][100/3580] Loss_D: 1.1284 Loss_G: 4.4243 D(x): 0.8197 D(G(z)): 0.5156 / 0.0253\n",
      "[0/25][200/3580] Loss_D: 0.3961 Loss_G: 4.1158 D(x): 0.7515 D(G(z)): 0.0556 / 0.0269\n",
      "[0/25][300/3580] Loss_D: 0.9674 Loss_G: 2.8537 D(x): 0.5981 D(G(z)): 0.2427 / 0.0858\n",
      "[0/25][400/3580] Loss_D: 0.4811 Loss_G: 3.4625 D(x): 0.7904 D(G(z)): 0.1854 / 0.0488\n",
      "[0/25][500/3580] Loss_D: 0.2596 Loss_G: 3.6539 D(x): 0.9100 D(G(z)): 0.1396 / 0.0369\n",
      "[0/25][600/3580] Loss_D: 1.6810 Loss_G: 3.8133 D(x): 0.2879 D(G(z)): 0.0028 / 0.0755\n",
      "[0/25][700/3580] Loss_D: 0.3891 Loss_G: 3.0534 D(x): 0.8441 D(G(z)): 0.1653 / 0.0741\n",
      "[0/25][800/3580] Loss_D: 0.3209 Loss_G: 5.0191 D(x): 0.9315 D(G(z)): 0.1971 / 0.0100\n",
      "[0/25][900/3580] Loss_D: 0.3619 Loss_G: 4.4613 D(x): 0.8200 D(G(z)): 0.1064 / 0.0196\n",
      "[0/25][1000/3580] Loss_D: 0.4146 Loss_G: 3.4354 D(x): 0.8033 D(G(z)): 0.1292 / 0.0566\n",
      "[0/25][1100/3580] Loss_D: 0.6140 Loss_G: 2.0235 D(x): 0.6455 D(G(z)): 0.0486 / 0.1863\n",
      "[0/25][1200/3580] Loss_D: 0.4373 Loss_G: 4.2644 D(x): 0.7966 D(G(z)): 0.1247 / 0.0264\n",
      "[0/25][1300/3580] Loss_D: 0.4487 Loss_G: 2.6231 D(x): 0.7208 D(G(z)): 0.0387 / 0.1102\n",
      "[0/25][1400/3580] Loss_D: 0.4221 Loss_G: 3.0560 D(x): 0.7979 D(G(z)): 0.1467 / 0.0724\n",
      "[0/25][1500/3580] Loss_D: 0.3339 Loss_G: 4.6690 D(x): 0.8812 D(G(z)): 0.1634 / 0.0156\n",
      "[0/25][1600/3580] Loss_D: 0.3139 Loss_G: 3.3514 D(x): 0.8158 D(G(z)): 0.0518 / 0.0553\n",
      "[0/25][1700/3580] Loss_D: 0.2581 Loss_G: 4.8146 D(x): 0.8289 D(G(z)): 0.0345 / 0.0159\n",
      "[0/25][1800/3580] Loss_D: 0.4490 Loss_G: 3.2605 D(x): 0.7149 D(G(z)): 0.0362 / 0.0639\n",
      "[0/25][1900/3580] Loss_D: 0.2350 Loss_G: 2.8973 D(x): 0.8638 D(G(z)): 0.0479 / 0.0993\n",
      "[0/25][2000/3580] Loss_D: 0.3927 Loss_G: 3.6809 D(x): 0.8134 D(G(z)): 0.1167 / 0.0374\n",
      "[0/25][2100/3580] Loss_D: 0.3529 Loss_G: 3.2013 D(x): 0.7745 D(G(z)): 0.0624 / 0.0627\n",
      "[0/25][2200/3580] Loss_D: 1.0635 Loss_G: 6.0739 D(x): 0.8339 D(G(z)): 0.4866 / 0.0040\n",
      "[0/25][2300/3580] Loss_D: 0.6316 Loss_G: 5.2972 D(x): 0.9458 D(G(z)): 0.3802 / 0.0079\n",
      "[0/25][2400/3580] Loss_D: 0.4475 Loss_G: 4.9410 D(x): 0.9428 D(G(z)): 0.2683 / 0.0127\n",
      "[0/25][2500/3580] Loss_D: 0.3951 Loss_G: 4.4621 D(x): 0.9020 D(G(z)): 0.2207 / 0.0183\n",
      "[0/25][2600/3580] Loss_D: 0.4118 Loss_G: 4.7304 D(x): 0.9041 D(G(z)): 0.2169 / 0.0146\n",
      "[0/25][2700/3580] Loss_D: 0.2525 Loss_G: 2.5434 D(x): 0.8783 D(G(z)): 0.0784 / 0.1387\n",
      "[0/25][2800/3580] Loss_D: 0.4006 Loss_G: 2.0949 D(x): 0.7393 D(G(z)): 0.0439 / 0.1648\n",
      "[0/25][2900/3580] Loss_D: 0.3733 Loss_G: 3.5923 D(x): 0.7396 D(G(z)): 0.0254 / 0.0483\n",
      "[0/25][3000/3580] Loss_D: 0.1722 Loss_G: 2.9790 D(x): 0.9083 D(G(z)): 0.0634 / 0.0863\n",
      "[0/25][3100/3580] Loss_D: 2.1695 Loss_G: 0.2325 D(x): 0.2112 D(G(z)): 0.0018 / 0.8089\n",
      "[0/25][3200/3580] Loss_D: 0.2509 Loss_G: 4.1882 D(x): 0.9102 D(G(z)): 0.1316 / 0.0232\n",
      "[0/25][3300/3580] Loss_D: 0.4311 Loss_G: 3.8017 D(x): 0.7349 D(G(z)): 0.0395 / 0.0469\n",
      "[0/25][3400/3580] Loss_D: 0.4107 Loss_G: 3.3318 D(x): 0.8507 D(G(z)): 0.1841 / 0.0520\n",
      "[0/25][3500/3580] Loss_D: 0.1738 Loss_G: 4.4957 D(x): 0.9136 D(G(z)): 0.0701 / 0.0170\n",
      "[1/25][0/3580] Loss_D: 3.1031 Loss_G: 9.3742 D(x): 0.9988 D(G(z)): 0.8891 / 0.0012\n",
      "[1/25][100/3580] Loss_D: 0.4202 Loss_G: 2.0015 D(x): 0.7412 D(G(z)): 0.0432 / 0.1965\n",
      "[1/25][200/3580] Loss_D: 0.2411 Loss_G: 5.0048 D(x): 0.9042 D(G(z)): 0.0951 / 0.0133\n",
      "[1/25][300/3580] Loss_D: 1.0792 Loss_G: 7.8212 D(x): 0.9576 D(G(z)): 0.5644 / 0.0007\n",
      "[1/25][400/3580] Loss_D: 0.1875 Loss_G: 3.0149 D(x): 0.8663 D(G(z)): 0.0329 / 0.0771\n",
      "[1/25][500/3580] Loss_D: 0.6399 Loss_G: 1.4444 D(x): 0.6604 D(G(z)): 0.0998 / 0.3046\n",
      "[1/25][600/3580] Loss_D: 0.3889 Loss_G: 4.0132 D(x): 0.8903 D(G(z)): 0.1826 / 0.0353\n",
      "[1/25][700/3580] Loss_D: 0.9393 Loss_G: 7.1056 D(x): 0.9563 D(G(z)): 0.5207 / 0.0016\n",
      "[1/25][800/3580] Loss_D: 0.1756 Loss_G: 3.3181 D(x): 0.9143 D(G(z)): 0.0683 / 0.0598\n",
      "[1/25][900/3580] Loss_D: 0.5801 Loss_G: 8.0412 D(x): 0.9617 D(G(z)): 0.3660 / 0.0008\n",
      "[1/25][1000/3580] Loss_D: 0.4523 Loss_G: 4.6700 D(x): 0.6824 D(G(z)): 0.0081 / 0.0174\n",
      "[1/25][1100/3580] Loss_D: 0.6055 Loss_G: 4.0889 D(x): 0.6241 D(G(z)): 0.0055 / 0.0338\n",
      "[1/25][1200/3580] Loss_D: 0.6174 Loss_G: 5.9046 D(x): 0.9652 D(G(z)): 0.3931 / 0.0039\n",
      "[1/25][1300/3580] Loss_D: 0.4553 Loss_G: 2.3478 D(x): 0.7040 D(G(z)): 0.0537 / 0.1410\n",
      "[1/25][1400/3580] Loss_D: 0.4438 Loss_G: 5.0469 D(x): 0.9416 D(G(z)): 0.2761 / 0.0093\n",
      "[1/25][1500/3580] Loss_D: 0.2069 Loss_G: 2.6782 D(x): 0.8980 D(G(z)): 0.0845 / 0.0994\n",
      "[1/25][1600/3580] Loss_D: 0.3930 Loss_G: 3.8346 D(x): 0.8672 D(G(z)): 0.1834 / 0.0341\n",
      "[1/25][1700/3580] Loss_D: 0.2242 Loss_G: 3.7222 D(x): 0.8967 D(G(z)): 0.0955 / 0.0347\n",
      "[1/25][1800/3580] Loss_D: 0.3458 Loss_G: 3.8607 D(x): 0.8744 D(G(z)): 0.1573 / 0.0314\n",
      "[1/25][1900/3580] Loss_D: 0.4113 Loss_G: 2.6772 D(x): 0.7351 D(G(z)): 0.0430 / 0.1042\n",
      "[1/25][2000/3580] Loss_D: 1.9184 Loss_G: 8.5682 D(x): 0.9975 D(G(z)): 0.7950 / 0.0005\n",
      "[1/25][2100/3580] Loss_D: 0.6026 Loss_G: 4.0711 D(x): 0.9207 D(G(z)): 0.3454 / 0.0268\n",
      "[1/25][2200/3580] Loss_D: 0.2544 Loss_G: 4.0470 D(x): 0.9135 D(G(z)): 0.1378 / 0.0242\n",
      "[1/25][2300/3580] Loss_D: 0.3220 Loss_G: 4.6451 D(x): 0.8988 D(G(z)): 0.1529 / 0.0195\n",
      "[1/25][2400/3580] Loss_D: 0.3352 Loss_G: 3.9980 D(x): 0.8916 D(G(z)): 0.1700 / 0.0268\n",
      "[1/25][2500/3580] Loss_D: 1.2067 Loss_G: 1.8350 D(x): 0.4277 D(G(z)): 0.0348 / 0.2645\n",
      "[1/25][2600/3580] Loss_D: 0.2320 Loss_G: 3.0560 D(x): 0.8323 D(G(z)): 0.0199 / 0.0700\n",
      "[1/25][2700/3580] Loss_D: 0.2832 Loss_G: 2.8069 D(x): 0.8296 D(G(z)): 0.0748 / 0.0788\n",
      "[1/25][2800/3580] Loss_D: 0.3693 Loss_G: 3.9647 D(x): 0.9208 D(G(z)): 0.2300 / 0.0243\n",
      "[1/25][2900/3580] Loss_D: 0.5091 Loss_G: 1.7098 D(x): 0.7215 D(G(z)): 0.0496 / 0.2625\n",
      "[1/25][3000/3580] Loss_D: 0.2754 Loss_G: 4.6369 D(x): 0.9653 D(G(z)): 0.1940 / 0.0133\n",
      "[1/25][3100/3580] Loss_D: 0.1360 Loss_G: 3.9820 D(x): 0.9656 D(G(z)): 0.0898 / 0.0278\n",
      "[1/25][3200/3580] Loss_D: 0.3459 Loss_G: 2.2106 D(x): 0.8131 D(G(z)): 0.0968 / 0.1499\n",
      "[1/25][3300/3580] Loss_D: 0.2982 Loss_G: 2.4935 D(x): 0.8519 D(G(z)): 0.1004 / 0.1134\n",
      "[1/25][3400/3580] Loss_D: 0.1836 Loss_G: 3.3835 D(x): 0.8756 D(G(z)): 0.0416 / 0.0484\n",
      "[1/25][3500/3580] Loss_D: 0.2643 Loss_G: 4.9824 D(x): 0.9693 D(G(z)): 0.1856 / 0.0095\n",
      "[2/25][0/3580] Loss_D: 0.1324 Loss_G: 3.4224 D(x): 0.9308 D(G(z)): 0.0497 / 0.0545\n",
      "[2/25][100/3580] Loss_D: 0.1871 Loss_G: 3.4192 D(x): 0.8754 D(G(z)): 0.0431 / 0.0541\n",
      "[2/25][200/3580] Loss_D: 0.2297 Loss_G: 2.7944 D(x): 0.8541 D(G(z)): 0.0501 / 0.1013\n",
      "[2/25][300/3580] Loss_D: 0.5895 Loss_G: 6.1849 D(x): 0.9840 D(G(z)): 0.3917 / 0.0028\n",
      "[2/25][400/3580] Loss_D: 0.1890 Loss_G: 4.9945 D(x): 0.9543 D(G(z)): 0.1224 / 0.0105\n",
      "[2/25][500/3580] Loss_D: 0.1613 Loss_G: 4.2924 D(x): 0.9500 D(G(z)): 0.0977 / 0.0212\n",
      "[2/25][600/3580] Loss_D: 0.1937 Loss_G: 3.9031 D(x): 0.9648 D(G(z)): 0.1383 / 0.0275\n",
      "[2/25][700/3580] Loss_D: 0.2737 Loss_G: 4.3866 D(x): 0.9615 D(G(z)): 0.1930 / 0.0158\n",
      "[2/25][800/3580] Loss_D: 0.2151 Loss_G: 3.7917 D(x): 0.8987 D(G(z)): 0.0783 / 0.0383\n",
      "[2/25][900/3580] Loss_D: 0.1379 Loss_G: 4.1777 D(x): 0.9328 D(G(z)): 0.0601 / 0.0263\n",
      "[2/25][1000/3580] Loss_D: 0.5427 Loss_G: 6.5195 D(x): 0.9894 D(G(z)): 0.3577 / 0.0026\n",
      "[2/25][1100/3580] Loss_D: 0.2980 Loss_G: 3.5579 D(x): 0.9268 D(G(z)): 0.1760 / 0.0402\n",
      "[2/25][1200/3580] Loss_D: 0.1880 Loss_G: 3.8298 D(x): 0.8708 D(G(z)): 0.0366 / 0.0359\n",
      "[2/25][1300/3580] Loss_D: 0.4409 Loss_G: 2.5566 D(x): 0.7044 D(G(z)): 0.0162 / 0.1227\n",
      "[2/25][1400/3580] Loss_D: 1.0682 Loss_G: 7.4513 D(x): 0.9840 D(G(z)): 0.5560 / 0.0010\n",
      "[2/25][1500/3580] Loss_D: 0.3332 Loss_G: 3.8800 D(x): 0.9232 D(G(z)): 0.1936 / 0.0303\n",
      "[2/25][1600/3580] Loss_D: 0.1446 Loss_G: 4.0257 D(x): 0.9550 D(G(z)): 0.0875 / 0.0259\n",
      "[2/25][1700/3580] Loss_D: 0.1579 Loss_G: 4.7164 D(x): 0.9513 D(G(z)): 0.0951 / 0.0155\n",
      "[2/25][1800/3580] Loss_D: 2.0737 Loss_G: 10.2967 D(x): 0.9943 D(G(z)): 0.7898 / 0.0001\n",
      "[2/25][1900/3580] Loss_D: 0.3382 Loss_G: 4.4004 D(x): 0.9831 D(G(z)): 0.2491 / 0.0169\n",
      "[2/25][2000/3580] Loss_D: 0.2484 Loss_G: 3.4604 D(x): 0.8407 D(G(z)): 0.0389 / 0.0532\n",
      "[2/25][2100/3580] Loss_D: 0.8883 Loss_G: 8.0551 D(x): 0.9696 D(G(z)): 0.5017 / 0.0005\n",
      "[2/25][2200/3580] Loss_D: 0.2209 Loss_G: 3.2221 D(x): 0.8884 D(G(z)): 0.0704 / 0.0563\n",
      "[2/25][2300/3580] Loss_D: 0.2330 Loss_G: 3.9257 D(x): 0.9459 D(G(z)): 0.1444 / 0.0299\n",
      "[2/25][2400/3580] Loss_D: 0.8294 Loss_G: 8.1285 D(x): 0.9963 D(G(z)): 0.4889 / 0.0004\n",
      "[2/25][2500/3580] Loss_D: 0.2745 Loss_G: 4.1761 D(x): 0.9185 D(G(z)): 0.1503 / 0.0233\n",
      "[2/25][2600/3580] Loss_D: 0.1702 Loss_G: 4.3448 D(x): 0.8845 D(G(z)): 0.0345 / 0.0212\n",
      "[2/25][2700/3580] Loss_D: 0.3280 Loss_G: 3.5034 D(x): 0.8782 D(G(z)): 0.1345 / 0.0615\n",
      "[2/25][2800/3580] Loss_D: 0.4563 Loss_G: 2.2333 D(x): 0.7226 D(G(z)): 0.0460 / 0.1588\n",
      "[2/25][2900/3580] Loss_D: 0.5820 Loss_G: 5.7159 D(x): 0.9529 D(G(z)): 0.3614 / 0.0046\n",
      "[2/25][3000/3580] Loss_D: 0.7698 Loss_G: 3.0710 D(x): 0.5879 D(G(z)): 0.0325 / 0.0927\n",
      "[2/25][3100/3580] Loss_D: 0.2389 Loss_G: 4.1686 D(x): 0.9893 D(G(z)): 0.1855 / 0.0233\n",
      "[2/25][3200/3580] Loss_D: 0.2707 Loss_G: 3.9892 D(x): 0.9708 D(G(z)): 0.1957 / 0.0257\n",
      "[2/25][3300/3580] Loss_D: 0.2199 Loss_G: 4.0000 D(x): 0.9473 D(G(z)): 0.1416 / 0.0247\n",
      "[2/25][3400/3580] Loss_D: 1.2937 Loss_G: 8.4164 D(x): 0.9784 D(G(z)): 0.6068 / 0.0005\n",
      "[2/25][3500/3580] Loss_D: 0.2781 Loss_G: 3.1010 D(x): 0.8049 D(G(z)): 0.0343 / 0.0673\n",
      "[3/25][0/3580] Loss_D: 0.1904 Loss_G: 4.1354 D(x): 0.9177 D(G(z)): 0.0893 / 0.0245\n",
      "[3/25][100/3580] Loss_D: 0.5708 Loss_G: 5.6308 D(x): 0.9871 D(G(z)): 0.3665 / 0.0053\n",
      "[3/25][200/3580] Loss_D: 0.1764 Loss_G: 4.5844 D(x): 0.9657 D(G(z)): 0.1198 / 0.0155\n",
      "[3/25][300/3580] Loss_D: 0.1347 Loss_G: 3.7334 D(x): 0.9577 D(G(z)): 0.0800 / 0.0355\n",
      "[3/25][400/3580] Loss_D: 0.4977 Loss_G: 4.6916 D(x): 0.9150 D(G(z)): 0.2804 / 0.0153\n",
      "[3/25][500/3580] Loss_D: 0.5744 Loss_G: 6.0522 D(x): 0.9897 D(G(z)): 0.3564 / 0.0037\n",
      "[3/25][600/3580] Loss_D: 0.4436 Loss_G: 6.7479 D(x): 0.9841 D(G(z)): 0.3156 / 0.0016\n",
      "[3/25][700/3580] Loss_D: 0.9539 Loss_G: 1.8522 D(x): 0.6133 D(G(z)): 0.1879 / 0.2096\n",
      "[3/25][800/3580] Loss_D: 0.1636 Loss_G: 3.3815 D(x): 0.9090 D(G(z)): 0.0564 / 0.0488\n",
      "[3/25][900/3580] Loss_D: 2.5331 Loss_G: 0.0201 D(x): 0.1357 D(G(z)): 0.0011 / 0.9806\n",
      "[3/25][1000/3580] Loss_D: 0.2879 Loss_G: 3.9960 D(x): 0.9375 D(G(z)): 0.1757 / 0.0271\n",
      "[3/25][1100/3580] Loss_D: 0.1836 Loss_G: 3.4376 D(x): 0.8869 D(G(z)): 0.0549 / 0.0517\n",
      "[3/25][1200/3580] Loss_D: 0.1175 Loss_G: 3.9779 D(x): 0.9325 D(G(z)): 0.0416 / 0.0293\n",
      "[3/25][1300/3580] Loss_D: 0.3405 Loss_G: 4.5466 D(x): 0.9380 D(G(z)): 0.1972 / 0.0146\n",
      "[3/25][1400/3580] Loss_D: 0.1533 Loss_G: 3.3918 D(x): 0.8955 D(G(z)): 0.0335 / 0.0516\n",
      "[3/25][1500/3580] Loss_D: 0.4301 Loss_G: 3.7463 D(x): 0.8430 D(G(z)): 0.1792 / 0.0410\n",
      "[3/25][1600/3580] Loss_D: 0.2715 Loss_G: 3.2929 D(x): 0.8220 D(G(z)): 0.0402 / 0.0565\n",
      "[3/25][1700/3580] Loss_D: 0.1204 Loss_G: 3.8354 D(x): 0.9305 D(G(z)): 0.0421 / 0.0324\n",
      "[3/25][1800/3580] Loss_D: 0.7228 Loss_G: 1.2533 D(x): 0.5658 D(G(z)): 0.0026 / 0.3653\n",
      "[3/25][1900/3580] Loss_D: 0.1085 Loss_G: 4.4496 D(x): 0.9315 D(G(z)): 0.0329 / 0.0222\n",
      "[3/25][2000/3580] Loss_D: 0.1296 Loss_G: 4.5528 D(x): 0.9504 D(G(z)): 0.0630 / 0.0194\n",
      "[3/25][2100/3580] Loss_D: 0.3043 Loss_G: 3.2058 D(x): 0.8963 D(G(z)): 0.1382 / 0.0680\n",
      "[3/25][2200/3580] Loss_D: 0.1860 Loss_G: 3.3634 D(x): 0.9127 D(G(z)): 0.0746 / 0.0472\n",
      "[3/25][2300/3580] Loss_D: 0.1813 Loss_G: 4.8464 D(x): 0.9750 D(G(z)): 0.1317 / 0.0106\n",
      "[3/25][2400/3580] Loss_D: 0.1946 Loss_G: 4.2668 D(x): 0.8670 D(G(z)): 0.0322 / 0.0299\n",
      "[3/25][2500/3580] Loss_D: 0.1909 Loss_G: 3.9275 D(x): 0.8722 D(G(z)): 0.0399 / 0.0343\n",
      "[3/25][2600/3580] Loss_D: 0.1299 Loss_G: 5.0897 D(x): 0.9373 D(G(z)): 0.0556 / 0.0130\n",
      "[3/25][2700/3580] Loss_D: 0.0981 Loss_G: 4.5494 D(x): 0.9689 D(G(z)): 0.0597 / 0.0170\n",
      "[3/25][2800/3580] Loss_D: 0.1801 Loss_G: 3.3962 D(x): 0.8665 D(G(z)): 0.0246 / 0.0542\n",
      "[3/25][2900/3580] Loss_D: 0.2801 Loss_G: 3.6153 D(x): 0.8010 D(G(z)): 0.0258 / 0.0458\n",
      "[3/25][3000/3580] Loss_D: 0.3387 Loss_G: 2.6977 D(x): 0.7601 D(G(z)): 0.0182 / 0.1062\n",
      "[3/25][3100/3580] Loss_D: 0.1121 Loss_G: 4.2011 D(x): 0.9527 D(G(z)): 0.0574 / 0.0237\n",
      "[3/25][3200/3580] Loss_D: 0.1378 Loss_G: 3.6236 D(x): 0.9246 D(G(z)): 0.0493 / 0.0428\n",
      "[3/25][3300/3580] Loss_D: 0.2704 Loss_G: 3.7171 D(x): 0.9206 D(G(z)): 0.1454 / 0.0405\n",
      "[3/25][3400/3580] Loss_D: 0.1953 Loss_G: 3.8824 D(x): 0.9421 D(G(z)): 0.1191 / 0.0272\n",
      "[3/25][3500/3580] Loss_D: 0.1039 Loss_G: 4.7687 D(x): 0.9210 D(G(z)): 0.0170 / 0.0182\n",
      "[4/25][0/3580] Loss_D: 2.0607 Loss_G: 0.2004 D(x): 0.2272 D(G(z)): 0.0047 / 0.8346\n",
      "[4/25][100/3580] Loss_D: 1.3543 Loss_G: 9.5851 D(x): 0.9787 D(G(z)): 0.6515 / 0.0002\n",
      "[4/25][200/3580] Loss_D: 0.2307 Loss_G: 4.8310 D(x): 0.9677 D(G(z)): 0.1549 / 0.0121\n",
      "[4/25][300/3580] Loss_D: 0.1010 Loss_G: 4.1243 D(x): 0.9625 D(G(z)): 0.0563 / 0.0242\n",
      "[4/25][400/3580] Loss_D: 0.1678 Loss_G: 4.3379 D(x): 0.9515 D(G(z)): 0.0995 / 0.0218\n",
      "[4/25][500/3580] Loss_D: 1.4702 Loss_G: 6.5885 D(x): 0.9562 D(G(z)): 0.6655 / 0.0024\n",
      "[4/25][600/3580] Loss_D: 0.1817 Loss_G: 4.0140 D(x): 0.9756 D(G(z)): 0.1302 / 0.0275\n",
      "[4/25][700/3580] Loss_D: 0.1401 Loss_G: 4.3237 D(x): 0.9908 D(G(z)): 0.1135 / 0.0196\n",
      "[4/25][800/3580] Loss_D: 0.5764 Loss_G: 2.6999 D(x): 0.6936 D(G(z)): 0.0928 / 0.1275\n",
      "[4/25][900/3580] Loss_D: 0.1338 Loss_G: 3.8717 D(x): 0.9682 D(G(z)): 0.0900 / 0.0310\n",
      "[4/25][1000/3580] Loss_D: 3.1517 Loss_G: 6.6294 D(x): 0.9856 D(G(z)): 0.8824 / 0.0043\n",
      "[4/25][1100/3580] Loss_D: 0.1934 Loss_G: 3.6359 D(x): 0.9067 D(G(z)): 0.0770 / 0.0338\n",
      "[4/25][1200/3580] Loss_D: 0.2450 Loss_G: 3.5269 D(x): 0.8504 D(G(z)): 0.0617 / 0.0530\n",
      "[4/25][1300/3580] Loss_D: 0.1171 Loss_G: 4.4070 D(x): 0.9597 D(G(z)): 0.0688 / 0.0200\n",
      "[4/25][1400/3580] Loss_D: 0.1410 Loss_G: 3.7564 D(x): 0.9324 D(G(z)): 0.0617 / 0.0365\n",
      "[4/25][1500/3580] Loss_D: 0.4811 Loss_G: 5.8006 D(x): 0.9522 D(G(z)): 0.2783 / 0.0051\n",
      "[4/25][1600/3580] Loss_D: 0.2902 Loss_G: 2.3760 D(x): 0.8165 D(G(z)): 0.0502 / 0.1303\n",
      "[4/25][1700/3580] Loss_D: 0.0796 Loss_G: 4.7700 D(x): 0.9725 D(G(z)): 0.0467 / 0.0131\n",
      "[4/25][1800/3580] Loss_D: 0.1036 Loss_G: 4.4865 D(x): 0.9577 D(G(z)): 0.0528 / 0.0203\n",
      "[4/25][1900/3580] Loss_D: 0.2137 Loss_G: 3.4945 D(x): 0.8692 D(G(z)): 0.0476 / 0.0558\n",
      "[4/25][2000/3580] Loss_D: 0.1298 Loss_G: 4.4418 D(x): 0.9179 D(G(z)): 0.0324 / 0.0205\n",
      "[4/25][2100/3580] Loss_D: 0.0670 Loss_G: 4.8524 D(x): 0.9573 D(G(z)): 0.0202 / 0.0142\n",
      "[4/25][2200/3580] Loss_D: 0.1479 Loss_G: 4.7153 D(x): 0.8911 D(G(z)): 0.0207 / 0.0164\n",
      "[4/25][2300/3580] Loss_D: 0.1502 Loss_G: 4.1226 D(x): 0.9114 D(G(z)): 0.0478 / 0.0274\n",
      "[4/25][2400/3580] Loss_D: 0.1331 Loss_G: 3.0153 D(x): 0.9280 D(G(z)): 0.0475 / 0.0902\n",
      "[4/25][2500/3580] Loss_D: 0.2355 Loss_G: 4.2268 D(x): 0.9118 D(G(z)): 0.1125 / 0.0268\n",
      "[4/25][2600/3580] Loss_D: 0.0824 Loss_G: 4.5929 D(x): 0.9720 D(G(z)): 0.0494 / 0.0174\n",
      "[4/25][2700/3580] Loss_D: 0.0885 Loss_G: 4.1719 D(x): 0.9593 D(G(z)): 0.0427 / 0.0262\n",
      "[4/25][2800/3580] Loss_D: 0.2186 Loss_G: 4.8263 D(x): 0.9880 D(G(z)): 0.1637 / 0.0113\n",
      "[4/25][2900/3580] Loss_D: 0.1508 Loss_G: 3.9677 D(x): 0.8890 D(G(z)): 0.0197 / 0.0341\n",
      "[4/25][3000/3580] Loss_D: 0.0512 Loss_G: 4.3964 D(x): 0.9715 D(G(z)): 0.0212 / 0.0217\n",
      "[4/25][3100/3580] Loss_D: 0.3629 Loss_G: 4.1670 D(x): 0.9301 D(G(z)): 0.2091 / 0.0212\n",
      "[4/25][3200/3580] Loss_D: 0.0953 Loss_G: 4.0277 D(x): 0.9438 D(G(z)): 0.0320 / 0.0324\n",
      "[4/25][3300/3580] Loss_D: 0.1111 Loss_G: 5.0761 D(x): 0.9570 D(G(z)): 0.0577 / 0.0125\n",
      "[4/25][3400/3580] Loss_D: 0.0931 Loss_G: 4.8746 D(x): 0.9299 D(G(z)): 0.0178 / 0.0154\n",
      "[4/25][3500/3580] Loss_D: 0.2714 Loss_G: 2.1611 D(x): 0.8029 D(G(z)): 0.0152 / 0.1849\n",
      "[5/25][0/3580] Loss_D: 0.1476 Loss_G: 3.8031 D(x): 0.8906 D(G(z)): 0.0178 / 0.0421\n",
      "[5/25][100/3580] Loss_D: 0.1153 Loss_G: 3.6135 D(x): 0.9354 D(G(z)): 0.0434 / 0.0399\n",
      "[5/25][200/3580] Loss_D: 0.2579 Loss_G: 2.5514 D(x): 0.8898 D(G(z)): 0.1086 / 0.1103\n",
      "[5/25][300/3580] Loss_D: 0.2844 Loss_G: 2.4921 D(x): 0.8164 D(G(z)): 0.0483 / 0.1349\n",
      "[5/25][400/3580] Loss_D: 0.1061 Loss_G: 4.3898 D(x): 0.9409 D(G(z)): 0.0387 / 0.0211\n",
      "[5/25][500/3580] Loss_D: 0.1309 Loss_G: 3.9947 D(x): 0.9438 D(G(z)): 0.0662 / 0.0279\n",
      "[5/25][600/3580] Loss_D: 0.3873 Loss_G: 5.2719 D(x): 0.9290 D(G(z)): 0.2391 / 0.0081\n",
      "[5/25][700/3580] Loss_D: 0.1947 Loss_G: 3.4771 D(x): 0.9002 D(G(z)): 0.0663 / 0.0529\n",
      "[5/25][800/3580] Loss_D: 0.1154 Loss_G: 3.7469 D(x): 0.9590 D(G(z)): 0.0655 / 0.0360\n",
      "[5/25][900/3580] Loss_D: 0.1371 Loss_G: 5.5602 D(x): 0.9761 D(G(z)): 0.0969 / 0.0065\n",
      "[5/25][1000/3580] Loss_D: 0.1216 Loss_G: 4.6239 D(x): 0.9235 D(G(z)): 0.0352 / 0.0224\n",
      "[5/25][1100/3580] Loss_D: 0.1134 Loss_G: 5.2007 D(x): 0.9704 D(G(z)): 0.0729 / 0.0095\n",
      "[5/25][1200/3580] Loss_D: 0.1078 Loss_G: 4.2240 D(x): 0.9305 D(G(z)): 0.0271 / 0.0243\n",
      "[5/25][1300/3580] Loss_D: 0.0782 Loss_G: 4.8449 D(x): 0.9843 D(G(z)): 0.0584 / 0.0103\n",
      "[5/25][1400/3580] Loss_D: 1.1018 Loss_G: 0.5889 D(x): 0.4937 D(G(z)): 0.1771 / 0.6354\n",
      "[5/25][1500/3580] Loss_D: 0.1913 Loss_G: 3.3322 D(x): 0.9562 D(G(z)): 0.1162 / 0.0607\n",
      "[5/25][1600/3580] Loss_D: 0.4307 Loss_G: 5.6185 D(x): 0.9968 D(G(z)): 0.3025 / 0.0049\n",
      "[5/25][1700/3580] Loss_D: 0.2427 Loss_G: 2.8535 D(x): 0.8341 D(G(z)): 0.0394 / 0.0866\n",
      "[5/25][1800/3580] Loss_D: 0.0937 Loss_G: 3.5520 D(x): 0.9500 D(G(z)): 0.0371 / 0.0475\n",
      "[5/25][1900/3580] Loss_D: 0.1240 Loss_G: 3.3834 D(x): 0.9307 D(G(z)): 0.0454 / 0.0580\n",
      "[5/25][2000/3580] Loss_D: 0.4112 Loss_G: 2.5747 D(x): 0.7700 D(G(z)): 0.0784 / 0.1065\n",
      "[5/25][2100/3580] Loss_D: 0.2938 Loss_G: 2.6774 D(x): 0.8096 D(G(z)): 0.0399 / 0.1279\n",
      "[5/25][2200/3580] Loss_D: 0.0811 Loss_G: 4.7767 D(x): 0.9723 D(G(z)): 0.0478 / 0.0142\n",
      "[5/25][2300/3580] Loss_D: 0.0896 Loss_G: 4.7831 D(x): 0.9659 D(G(z)): 0.0491 / 0.0133\n",
      "[5/25][2400/3580] Loss_D: 0.1855 Loss_G: 4.7198 D(x): 0.9430 D(G(z)): 0.1043 / 0.0150\n",
      "[5/25][2500/3580] Loss_D: 0.1460 Loss_G: 3.2276 D(x): 0.8945 D(G(z)): 0.0231 / 0.0680\n",
      "[5/25][2600/3580] Loss_D: 0.0993 Loss_G: 4.1227 D(x): 0.9451 D(G(z)): 0.0380 / 0.0272\n",
      "[5/25][2700/3580] Loss_D: 0.0998 Loss_G: 4.5340 D(x): 0.9881 D(G(z)): 0.0806 / 0.0141\n",
      "[5/25][2800/3580] Loss_D: 0.0855 Loss_G: 4.5099 D(x): 0.9779 D(G(z)): 0.0590 / 0.0179\n",
      "[5/25][2900/3580] Loss_D: 0.2087 Loss_G: 3.6775 D(x): 0.8849 D(G(z)): 0.0462 / 0.0525\n",
      "[5/25][3000/3580] Loss_D: 0.1129 Loss_G: 4.2018 D(x): 0.9249 D(G(z)): 0.0289 / 0.0284\n",
      "[5/25][3100/3580] Loss_D: 0.1271 Loss_G: 4.3533 D(x): 0.9079 D(G(z)): 0.0202 / 0.0252\n",
      "[5/25][3200/3580] Loss_D: 0.1251 Loss_G: 4.2075 D(x): 0.9621 D(G(z)): 0.0782 / 0.0210\n",
      "[5/25][3300/3580] Loss_D: 0.1432 Loss_G: 4.0188 D(x): 0.9264 D(G(z)): 0.0530 / 0.0294\n",
      "[5/25][3400/3580] Loss_D: 0.0765 Loss_G: 4.9064 D(x): 0.9656 D(G(z)): 0.0372 / 0.0167\n",
      "[5/25][3500/3580] Loss_D: 0.0493 Loss_G: 5.4068 D(x): 0.9689 D(G(z)): 0.0166 / 0.0087\n",
      "[6/25][0/3580] Loss_D: 0.1486 Loss_G: 5.8458 D(x): 0.9889 D(G(z)): 0.1136 / 0.0044\n",
      "[6/25][100/3580] Loss_D: 0.2074 Loss_G: 1.7263 D(x): 0.8519 D(G(z)): 0.0266 / 0.2614\n",
      "[6/25][200/3580] Loss_D: 0.1820 Loss_G: 3.7582 D(x): 0.9472 D(G(z)): 0.1047 / 0.0379\n",
      "[6/25][300/3580] Loss_D: 0.1356 Loss_G: 3.9849 D(x): 0.9014 D(G(z)): 0.0195 / 0.0335\n",
      "[6/25][400/3580] Loss_D: 0.1225 Loss_G: 3.4394 D(x): 0.9109 D(G(z)): 0.0228 / 0.0501\n",
      "[6/25][500/3580] Loss_D: 0.4145 Loss_G: 3.2736 D(x): 0.9508 D(G(z)): 0.2501 / 0.0546\n",
      "[6/25][600/3580] Loss_D: 0.0347 Loss_G: 5.0060 D(x): 0.9842 D(G(z)): 0.0179 / 0.0143\n",
      "[6/25][700/3580] Loss_D: 0.0928 Loss_G: 4.3719 D(x): 0.9484 D(G(z)): 0.0361 / 0.0247\n",
      "[6/25][800/3580] Loss_D: 0.0568 Loss_G: 4.6243 D(x): 0.9879 D(G(z)): 0.0417 / 0.0180\n",
      "[6/25][900/3580] Loss_D: 1.0933 Loss_G: 5.3343 D(x): 0.9042 D(G(z)): 0.5037 / 0.0100\n",
      "[6/25][1000/3580] Loss_D: 0.1618 Loss_G: 4.0896 D(x): 0.9513 D(G(z)): 0.0969 / 0.0302\n",
      "[6/25][1100/3580] Loss_D: 0.0633 Loss_G: 5.4358 D(x): 0.9892 D(G(z)): 0.0467 / 0.0083\n",
      "[6/25][1200/3580] Loss_D: 0.4033 Loss_G: 2.2731 D(x): 0.7531 D(G(z)): 0.0462 / 0.1670\n",
      "[6/25][1300/3580] Loss_D: 0.1020 Loss_G: 4.0021 D(x): 0.9329 D(G(z)): 0.0246 / 0.0339\n",
      "[6/25][1400/3580] Loss_D: 0.1089 Loss_G: 3.7816 D(x): 0.9263 D(G(z)): 0.0246 / 0.0488\n",
      "[6/25][1500/3580] Loss_D: 0.6896 Loss_G: 6.0670 D(x): 0.9981 D(G(z)): 0.3983 / 0.0036\n",
      "[6/25][1600/3580] Loss_D: 0.0590 Loss_G: 4.5155 D(x): 0.9667 D(G(z)): 0.0229 / 0.0198\n",
      "[6/25][1700/3580] Loss_D: 0.0618 Loss_G: 4.6379 D(x): 0.9585 D(G(z)): 0.0165 / 0.0148\n",
      "[6/25][1800/3580] Loss_D: 0.0872 Loss_G: 4.9696 D(x): 0.9833 D(G(z)): 0.0624 / 0.0118\n",
      "[6/25][1900/3580] Loss_D: 0.0319 Loss_G: 5.1389 D(x): 0.9874 D(G(z)): 0.0184 / 0.0137\n",
      "[6/25][2000/3580] Loss_D: 0.3852 Loss_G: 1.0595 D(x): 0.7408 D(G(z)): 0.0045 / 0.4211\n",
      "[6/25][2100/3580] Loss_D: 4.8167 Loss_G: 10.8959 D(x): 0.9997 D(G(z)): 0.9781 / 0.0001\n",
      "[6/25][2200/3580] Loss_D: 0.1592 Loss_G: 3.9312 D(x): 0.9204 D(G(z)): 0.0540 / 0.0383\n",
      "[6/25][2300/3580] Loss_D: 0.2546 Loss_G: 2.2598 D(x): 0.8888 D(G(z)): 0.1109 / 0.1519\n",
      "[6/25][2400/3580] Loss_D: 1.1058 Loss_G: 0.2856 D(x): 0.4361 D(G(z)): 0.0249 / 0.7911\n",
      "[6/25][2500/3580] Loss_D: 0.1437 Loss_G: 3.9227 D(x): 0.9363 D(G(z)): 0.0653 / 0.0328\n",
      "[6/25][2600/3580] Loss_D: 0.2070 Loss_G: 3.5058 D(x): 0.8484 D(G(z)): 0.0212 / 0.0464\n",
      "[6/25][2700/3580] Loss_D: 0.2941 Loss_G: 1.8377 D(x): 0.7896 D(G(z)): 0.0123 / 0.2466\n",
      "[6/25][2800/3580] Loss_D: 0.0490 Loss_G: 4.6715 D(x): 0.9697 D(G(z)): 0.0173 / 0.0155\n",
      "[6/25][2900/3580] Loss_D: 0.3680 Loss_G: 2.6072 D(x): 0.7730 D(G(z)): 0.0407 / 0.1444\n",
      "[6/25][3000/3580] Loss_D: 0.2453 Loss_G: 2.9043 D(x): 0.8230 D(G(z)): 0.0106 / 0.0909\n",
      "[6/25][3100/3580] Loss_D: 0.0649 Loss_G: 4.5428 D(x): 0.9691 D(G(z)): 0.0297 / 0.0191\n",
      "[6/25][3200/3580] Loss_D: 0.1487 Loss_G: 3.6294 D(x): 0.8818 D(G(z)): 0.0094 / 0.0510\n",
      "[6/25][3300/3580] Loss_D: 0.1733 Loss_G: 5.4797 D(x): 0.9921 D(G(z)): 0.1293 / 0.0058\n",
      "[6/25][3400/3580] Loss_D: 0.0691 Loss_G: 5.0469 D(x): 0.9837 D(G(z)): 0.0486 / 0.0125\n",
      "[6/25][3500/3580] Loss_D: 0.5309 Loss_G: 3.9043 D(x): 0.8510 D(G(z)): 0.2419 / 0.0375\n",
      "[7/25][0/3580] Loss_D: 1.6087 Loss_G: 11.2330 D(x): 0.9985 D(G(z)): 0.6698 / 0.0000\n",
      "[7/25][100/3580] Loss_D: 0.1238 Loss_G: 4.5078 D(x): 0.9640 D(G(z)): 0.0697 / 0.0177\n",
      "[7/25][200/3580] Loss_D: 0.0419 Loss_G: 5.3448 D(x): 0.9754 D(G(z)): 0.0155 / 0.0094\n",
      "[7/25][300/3580] Loss_D: 0.0364 Loss_G: 5.9269 D(x): 0.9857 D(G(z)): 0.0209 / 0.0049\n",
      "[7/25][400/3580] Loss_D: 0.1460 Loss_G: 4.6559 D(x): 0.9557 D(G(z)): 0.0816 / 0.0168\n",
      "[7/25][500/3580] Loss_D: 0.2689 Loss_G: 2.7900 D(x): 0.9328 D(G(z)): 0.1514 / 0.0927\n",
      "[7/25][600/3580] Loss_D: 0.3944 Loss_G: 1.5944 D(x): 0.7413 D(G(z)): 0.0053 / 0.2977\n",
      "[7/25][700/3580] Loss_D: 0.1341 Loss_G: 5.9291 D(x): 0.9779 D(G(z)): 0.0943 / 0.0054\n",
      "[7/25][800/3580] Loss_D: 0.1107 Loss_G: 4.9877 D(x): 0.9938 D(G(z)): 0.0938 / 0.0101\n",
      "[7/25][900/3580] Loss_D: 0.0402 Loss_G: 5.8572 D(x): 0.9783 D(G(z)): 0.0168 / 0.0051\n",
      "[7/25][1000/3580] Loss_D: 0.0444 Loss_G: 4.7908 D(x): 0.9741 D(G(z)): 0.0171 / 0.0146\n",
      "[7/25][1100/3580] Loss_D: 0.3798 Loss_G: 5.5244 D(x): 0.9738 D(G(z)): 0.2397 / 0.0075\n",
      "[7/25][1200/3580] Loss_D: 0.0720 Loss_G: 4.6475 D(x): 0.9746 D(G(z)): 0.0408 / 0.0230\n",
      "[7/25][1300/3580] Loss_D: 0.0886 Loss_G: 5.8725 D(x): 0.9313 D(G(z)): 0.0052 / 0.0054\n",
      "[7/25][1400/3580] Loss_D: 0.0965 Loss_G: 4.2386 D(x): 0.9574 D(G(z)): 0.0461 / 0.0248\n",
      "[7/25][1500/3580] Loss_D: 0.0710 Loss_G: 5.2538 D(x): 0.9730 D(G(z)): 0.0388 / 0.0108\n",
      "[7/25][1600/3580] Loss_D: 0.7397 Loss_G: 9.7173 D(x): 0.9930 D(G(z)): 0.4164 / 0.0001\n",
      "[7/25][1700/3580] Loss_D: 0.1264 Loss_G: 3.8393 D(x): 0.9039 D(G(z)): 0.0155 / 0.0380\n",
      "[7/25][1800/3580] Loss_D: 0.0595 Loss_G: 5.1631 D(x): 0.9519 D(G(z)): 0.0074 / 0.0133\n",
      "[7/25][1900/3580] Loss_D: 2.6690 Loss_G: 0.4754 D(x): 0.1538 D(G(z)): 0.0004 / 0.6773\n",
      "[7/25][2000/3580] Loss_D: 0.1059 Loss_G: 3.7923 D(x): 0.9313 D(G(z)): 0.0283 / 0.0390\n",
      "[7/25][2100/3580] Loss_D: 0.5468 Loss_G: 7.6740 D(x): 0.9738 D(G(z)): 0.3464 / 0.0008\n",
      "[7/25][2200/3580] Loss_D: 0.1827 Loss_G: 2.9668 D(x): 0.8681 D(G(z)): 0.0249 / 0.0902\n",
      "[7/25][2300/3580] Loss_D: 0.0507 Loss_G: 4.6365 D(x): 0.9730 D(G(z)): 0.0215 / 0.0168\n",
      "[7/25][2400/3580] Loss_D: 0.1993 Loss_G: 4.6329 D(x): 0.9078 D(G(z)): 0.0672 / 0.0209\n",
      "[7/25][2500/3580] Loss_D: 3.2846 Loss_G: 1.3613 D(x): 0.0799 D(G(z)): 0.0012 / 0.3653\n",
      "[7/25][2600/3580] Loss_D: 0.0916 Loss_G: 5.7257 D(x): 0.9714 D(G(z)): 0.0542 / 0.0077\n",
      "[7/25][2700/3580] Loss_D: 0.0694 Loss_G: 5.4887 D(x): 0.9888 D(G(z)): 0.0523 / 0.0092\n",
      "[7/25][2800/3580] Loss_D: 0.1179 Loss_G: 3.1636 D(x): 0.9116 D(G(z)): 0.0152 / 0.0656\n",
      "[7/25][2900/3580] Loss_D: 0.0233 Loss_G: 5.6564 D(x): 0.9869 D(G(z)): 0.0097 / 0.0076\n",
      "[7/25][3000/3580] Loss_D: 0.0467 Loss_G: 5.0347 D(x): 0.9707 D(G(z)): 0.0153 / 0.0144\n",
      "[7/25][3100/3580] Loss_D: 0.0555 Loss_G: 4.6255 D(x): 0.9576 D(G(z)): 0.0105 / 0.0148\n",
      "[7/25][3200/3580] Loss_D: 0.0919 Loss_G: 6.0977 D(x): 0.9965 D(G(z)): 0.0788 / 0.0033\n",
      "[7/25][3300/3580] Loss_D: 0.0593 Loss_G: 4.4710 D(x): 0.9575 D(G(z)): 0.0127 / 0.0221\n",
      "[7/25][3400/3580] Loss_D: 0.2887 Loss_G: 4.2074 D(x): 0.8862 D(G(z)): 0.1195 / 0.0249\n",
      "[7/25][3500/3580] Loss_D: 0.5426 Loss_G: 5.9699 D(x): 0.9593 D(G(z)): 0.3153 / 0.0040\n",
      "[8/25][0/3580] Loss_D: 0.1387 Loss_G: 3.7904 D(x): 0.9088 D(G(z)): 0.0338 / 0.0393\n",
      "[8/25][100/3580] Loss_D: 0.0612 Loss_G: 4.7061 D(x): 0.9804 D(G(z)): 0.0364 / 0.0164\n",
      "[8/25][200/3580] Loss_D: 0.5288 Loss_G: 2.4786 D(x): 0.6468 D(G(z)): 0.0025 / 0.1653\n",
      "[8/25][300/3580] Loss_D: 0.4608 Loss_G: 7.5574 D(x): 0.9701 D(G(z)): 0.2730 / 0.0008\n",
      "[8/25][400/3580] Loss_D: 0.1252 Loss_G: 4.5523 D(x): 0.9208 D(G(z)): 0.0314 / 0.0217\n",
      "[8/25][500/3580] Loss_D: 0.0863 Loss_G: 4.4265 D(x): 0.9398 D(G(z)): 0.0200 / 0.0292\n",
      "[8/25][600/3580] Loss_D: 0.0343 Loss_G: 5.4628 D(x): 0.9879 D(G(z)): 0.0213 / 0.0079\n",
      "[8/25][700/3580] Loss_D: 0.0208 Loss_G: 5.5376 D(x): 0.9926 D(G(z)): 0.0130 / 0.0086\n",
      "[8/25][800/3580] Loss_D: 0.0284 Loss_G: 5.5032 D(x): 0.9871 D(G(z)): 0.0150 / 0.0088\n",
      "[8/25][900/3580] Loss_D: 0.3371 Loss_G: 0.5185 D(x): 0.7596 D(G(z)): 0.0008 / 0.6655\n",
      "[8/25][1000/3580] Loss_D: 0.1285 Loss_G: 5.5799 D(x): 0.9127 D(G(z)): 0.0173 / 0.0111\n",
      "[8/25][1100/3580] Loss_D: 0.0878 Loss_G: 5.2412 D(x): 0.9798 D(G(z)): 0.0607 / 0.0107\n",
      "[8/25][1200/3580] Loss_D: 0.1746 Loss_G: 3.4982 D(x): 0.8771 D(G(z)): 0.0164 / 0.0785\n",
      "[8/25][1300/3580] Loss_D: 0.1017 Loss_G: 4.6934 D(x): 0.9602 D(G(z)): 0.0527 / 0.0189\n",
      "[8/25][1400/3580] Loss_D: 0.0660 Loss_G: 4.6951 D(x): 0.9787 D(G(z)): 0.0415 / 0.0181\n",
      "[8/25][1500/3580] Loss_D: 0.1061 Loss_G: 4.1668 D(x): 0.9216 D(G(z)): 0.0128 / 0.0316\n",
      "[8/25][1600/3580] Loss_D: 0.0886 Loss_G: 5.8197 D(x): 0.9973 D(G(z)): 0.0775 / 0.0051\n",
      "[8/25][1700/3580] Loss_D: 0.0853 Loss_G: 3.9660 D(x): 0.9742 D(G(z)): 0.0518 / 0.0339\n",
      "[8/25][1800/3580] Loss_D: 0.0349 Loss_G: 5.7359 D(x): 0.9770 D(G(z)): 0.0109 / 0.0084\n",
      "[8/25][1900/3580] Loss_D: 0.0447 Loss_G: 6.0434 D(x): 0.9647 D(G(z)): 0.0069 / 0.0051\n",
      "[8/25][2000/3580] Loss_D: 0.0439 Loss_G: 5.6914 D(x): 0.9923 D(G(z)): 0.0326 / 0.0060\n",
      "[8/25][2100/3580] Loss_D: 0.0302 Loss_G: 6.1956 D(x): 0.9933 D(G(z)): 0.0218 / 0.0040\n",
      "[8/25][2200/3580] Loss_D: 0.0368 Loss_G: 6.9721 D(x): 0.9696 D(G(z)): 0.0038 / 0.0024\n",
      "[8/25][2300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][2400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][2500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][2600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][2700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][2800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][2900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][3000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][3100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][3200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][3300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][3400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[8/25][3500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][0/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][1900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][2900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][3000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][3100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][3200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][3300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][3400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[9/25][3500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][0/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][1900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][2900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][3000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][3100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][3200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][3300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][3400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[10/25][3500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][0/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][1900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][2900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][3000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][3100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][3200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][3300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][3400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[11/25][3500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][0/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][600/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][700/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][800/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][900/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][1000/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][1100/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][1200/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][1300/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][1400/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n",
      "[12/25][1500/3580] Loss_D: 27.6310 Loss_G: 0.0000 D(x): 1.0000 D(G(z)): 1.0000 / 1.0000\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0: \n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, niter, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFpOOkDJKKXb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DCGAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
